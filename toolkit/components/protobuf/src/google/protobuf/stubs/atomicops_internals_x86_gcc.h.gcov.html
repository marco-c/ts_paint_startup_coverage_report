<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - output.info - toolkit/components/protobuf/src/google/protobuf/stubs/atomicops_internals_x86_gcc.h</title>
  <link rel="stylesheet" type="text/css" href="../../../../../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../../../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../../../../../index.html">top level</a> - <a href="index.html">toolkit/components/protobuf/src/google/protobuf/stubs</a> - atomicops_internals_x86_gcc.h<span style="font-size: 80%;"> (source / <a href="atomicops_internals_x86_gcc.h.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">output.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">15</td>
            <td class="headerCovTableEntry">16</td>
            <td class="headerCovTableEntryHi">93.8 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2017-07-14 16:53:18</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">4</td>
            <td class="headerCovTableEntry">4</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Legend:</td>
            <td class="headerValueLeg">            Lines:
            <span class="coverLegendCov">hit</span>
            <span class="coverLegendNoCov">not hit</span>
</td>
            <td></td>
          </tr>
          <tr><td><img src="../../../../../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../../../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : // Protocol Buffers - Google's data interchange format</a>
<span class="lineNum">       2 </span>            : // Copyright 2012 Google Inc.  All rights reserved.
<span class="lineNum">       3 </span>            : // https://developers.google.com/protocol-buffers/
<span class="lineNum">       4 </span>            : //
<span class="lineNum">       5 </span>            : // Redistribution and use in source and binary forms, with or without
<span class="lineNum">       6 </span>            : // modification, are permitted provided that the following conditions are
<span class="lineNum">       7 </span>            : // met:
<span class="lineNum">       8 </span>            : //
<span class="lineNum">       9 </span>            : //     * Redistributions of source code must retain the above copyright
<span class="lineNum">      10 </span>            : // notice, this list of conditions and the following disclaimer.
<span class="lineNum">      11 </span>            : //     * Redistributions in binary form must reproduce the above
<span class="lineNum">      12 </span>            : // copyright notice, this list of conditions and the following disclaimer
<span class="lineNum">      13 </span>            : // in the documentation and/or other materials provided with the
<span class="lineNum">      14 </span>            : // distribution.
<span class="lineNum">      15 </span>            : //     * Neither the name of Google Inc. nor the names of its
<span class="lineNum">      16 </span>            : // contributors may be used to endorse or promote products derived from
<span class="lineNum">      17 </span>            : // this software without specific prior written permission.
<span class="lineNum">      18 </span>            : //
<span class="lineNum">      19 </span>            : // THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
<span class="lineNum">      20 </span>            : // &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
<span class="lineNum">      21 </span>            : // LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
<span class="lineNum">      22 </span>            : // A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
<span class="lineNum">      23 </span>            : // OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
<span class="lineNum">      24 </span>            : // SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
<span class="lineNum">      25 </span>            : // LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
<span class="lineNum">      26 </span>            : // DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
<span class="lineNum">      27 </span>            : // THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
<span class="lineNum">      28 </span>            : // (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
<span class="lineNum">      29 </span>            : // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
<span class="lineNum">      30 </span>            : 
<span class="lineNum">      31 </span>            : // This file is an internal atomic implementation, use atomicops.h instead.
<span class="lineNum">      32 </span>            : 
<span class="lineNum">      33 </span>            : #ifndef GOOGLE_PROTOBUF_ATOMICOPS_INTERNALS_X86_GCC_H_
<span class="lineNum">      34 </span>            : #define GOOGLE_PROTOBUF_ATOMICOPS_INTERNALS_X86_GCC_H_
<span class="lineNum">      35 </span>            : 
<span class="lineNum">      36 </span>            : namespace google {
<span class="lineNum">      37 </span>            : namespace protobuf {
<span class="lineNum">      38 </span>            : namespace internal {
<span class="lineNum">      39 </span>            : 
<span class="lineNum">      40 </span>            : // This struct is not part of the public API of this module; clients may not
<span class="lineNum">      41 </span>            : // use it.
<span class="lineNum">      42 </span>            : // Features of this x86.  Values may not be correct before main() is run,
<span class="lineNum">      43 </span>            : // but are set conservatively.
<span class="lineNum">      44 </span>            : struct AtomicOps_x86CPUFeatureStruct {
<span class="lineNum">      45 </span>            :   bool has_amd_lock_mb_bug;  // Processor has AMD memory-barrier bug; do lfence
<span class="lineNum">      46 </span>            :                              // after acquire compare-and-swap.
<span class="lineNum">      47 </span>            :   bool has_sse2;             // Processor has SSE2.
<span class="lineNum">      48 </span>            : };
<span class="lineNum">      49 </span>            : extern struct AtomicOps_x86CPUFeatureStruct AtomicOps_Internalx86CPUFeatures;
<span class="lineNum">      50 </span>            : 
<span class="lineNum">      51 </span>            : #define ATOMICOPS_COMPILER_BARRIER() __asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)
<span class="lineNum">      52 </span>            : 
<span class="lineNum">      53 </span>            : // 32-bit low-level operations on any platform.
<span class="lineNum">      54 </span>            : 
<span class="lineNum">      55 </span>            : inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr,
<span class="lineNum">      56 </span>            :                                          Atomic32 old_value,
<span class="lineNum">      57 </span>            :                                          Atomic32 new_value) {
<span class="lineNum">      58 </span>            :   Atomic32 prev;
<span class="lineNum">      59 </span>            :   __asm__ __volatile__(&quot;lock; cmpxchgl %1,%2&quot;
<span class="lineNum">      60 </span>            :                        : &quot;=a&quot; (prev)
<span class="lineNum">      61 </span>            :                        : &quot;q&quot; (new_value), &quot;m&quot; (*ptr), &quot;0&quot; (old_value)
<span class="lineNum">      62 </span>            :                        : &quot;memory&quot;);
<span class="lineNum">      63 </span>            :   return prev;
<span class="lineNum">      64 </span>            : }
<span class="lineNum">      65 </span>            : 
<span class="lineNum">      66 </span>            : inline Atomic32 NoBarrier_AtomicExchange(volatile Atomic32* ptr,
<span class="lineNum">      67 </span>            :                                          Atomic32 new_value) {
<span class="lineNum">      68 </span>            :   __asm__ __volatile__(&quot;xchgl %1,%0&quot;  // The lock prefix is implicit for xchg.
<span class="lineNum">      69 </span>            :                        : &quot;=r&quot; (new_value)
<span class="lineNum">      70 </span>            :                        : &quot;m&quot; (*ptr), &quot;0&quot; (new_value)
<span class="lineNum">      71 </span>            :                        : &quot;memory&quot;);
<span class="lineNum">      72 </span>            :   return new_value;  // Now it's the previous value.
<span class="lineNum">      73 </span>            : }
<span class="lineNum">      74 </span>            : 
<span class="lineNum">      75 </span>            : inline Atomic32 NoBarrier_AtomicIncrement(volatile Atomic32* ptr,
<span class="lineNum">      76 </span>            :                                           Atomic32 increment) {
<span class="lineNum">      77 </span>            :   Atomic32 temp = increment;
<span class="lineNum">      78 </span>            :   __asm__ __volatile__(&quot;lock; xaddl %0,%1&quot;
<span class="lineNum">      79 </span>            :                        : &quot;+r&quot; (temp), &quot;+m&quot; (*ptr)
<span class="lineNum">      80 </span>            :                        : : &quot;memory&quot;);
<span class="lineNum">      81 </span>            :   // temp now holds the old value of *ptr
<span class="lineNum">      82 </span>            :   return temp + increment;
<span class="lineNum">      83 </span>            : }
<span class="lineNum">      84 </span>            : 
<span class="lineNum">      85 </span>            : inline Atomic32 Barrier_AtomicIncrement(volatile Atomic32* ptr,
<span class="lineNum">      86 </span>            :                                         Atomic32 increment) {
<span class="lineNum">      87 </span>            :   Atomic32 temp = increment;
<span class="lineNum">      88 </span>            :   __asm__ __volatile__(&quot;lock; xaddl %0,%1&quot;
<span class="lineNum">      89 </span>            :                        : &quot;+r&quot; (temp), &quot;+m&quot; (*ptr)
<span class="lineNum">      90 </span>            :                        : : &quot;memory&quot;);
<span class="lineNum">      91 </span>            :   // temp now holds the old value of *ptr
<span class="lineNum">      92 </span>            :   if (AtomicOps_Internalx86CPUFeatures.has_amd_lock_mb_bug) {
<span class="lineNum">      93 </span>            :     __asm__ __volatile__(&quot;lfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">      94 </span>            :   }
<span class="lineNum">      95 </span>            :   return temp + increment;
<span class="lineNum">      96 </span>            : }
<span class="lineNum">      97 </span>            : 
<span class="lineNum">      98 </span>            : inline Atomic32 Acquire_CompareAndSwap(volatile Atomic32* ptr,
<span class="lineNum">      99 </span>            :                                        Atomic32 old_value,
<span class="lineNum">     100 </span>            :                                        Atomic32 new_value) {
<span class="lineNum">     101 </span>            :   Atomic32 x = NoBarrier_CompareAndSwap(ptr, old_value, new_value);
<span class="lineNum">     102 </span>            :   if (AtomicOps_Internalx86CPUFeatures.has_amd_lock_mb_bug) {
<span class="lineNum">     103 </span>            :     __asm__ __volatile__(&quot;lfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">     104 </span>            :   }
<span class="lineNum">     105 </span>            :   return x;
<span class="lineNum">     106 </span>            : }
<span class="lineNum">     107 </span>            : 
<span class="lineNum">     108 </span>            : inline Atomic32 Release_CompareAndSwap(volatile Atomic32* ptr,
<span class="lineNum">     109 </span>            :                                        Atomic32 old_value,
<span class="lineNum">     110 </span>            :                                        Atomic32 new_value) {
<span class="lineNum">     111 </span>            :   return NoBarrier_CompareAndSwap(ptr, old_value, new_value);
<span class="lineNum">     112 </span>            : }
<span class="lineNum">     113 </span>            : 
<span class="lineNum">     114 </span>            : inline void NoBarrier_Store(volatile Atomic32* ptr, Atomic32 value) {
<span class="lineNum">     115 </span>            :   *ptr = value;
<span class="lineNum">     116 </span>            : }
<span class="lineNum">     117 </span>            : 
<span class="lineNum">     118 </span>            : #if defined(__x86_64__)
<span class="lineNum">     119 </span>            : 
<span class="lineNum">     120 </span>            : // 64-bit implementations of memory barrier can be simpler, because it
<span class="lineNum">     121 </span>            : // &quot;mfence&quot; is guaranteed to exist.
<span class="lineNum">     122 </span>            : inline void MemoryBarrier() {
<span class="lineNum">     123 </span>            :   __asm__ __volatile__(&quot;mfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">     124 </span>            : }
<span class="lineNum">     125 </span>            : 
<span class="lineNum">     126 </span>            : inline void Acquire_Store(volatile Atomic32* ptr, Atomic32 value) {
<span class="lineNum">     127 </span>            :   *ptr = value;
<span class="lineNum">     128 </span>            :   MemoryBarrier();
<span class="lineNum">     129 </span>            : }
<span class="lineNum">     130 </span>            : 
<span class="lineNum">     131 </span>            : #else
<span class="lineNum">     132 </span>            : 
<span class="lineNum">     133 </span>            : inline void MemoryBarrier() {
<span class="lineNum">     134 </span>            :   if (AtomicOps_Internalx86CPUFeatures.has_sse2) {
<span class="lineNum">     135 </span>            :     __asm__ __volatile__(&quot;mfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">     136 </span>            :   } else {  // mfence is faster but not present on PIII
<span class="lineNum">     137 </span>            :     Atomic32 x = 0;
<span class="lineNum">     138 </span>            :     NoBarrier_AtomicExchange(&amp;x, 0);  // acts as a barrier on PIII
<span class="lineNum">     139 </span>            :   }
<span class="lineNum">     140 </span>            : }
<span class="lineNum">     141 </span>            : 
<span class="lineNum">     142 </span>            : inline void Acquire_Store(volatile Atomic32* ptr, Atomic32 value) {
<span class="lineNum">     143 </span>            :   if (AtomicOps_Internalx86CPUFeatures.has_sse2) {
<span class="lineNum">     144 </span>            :     *ptr = value;
<span class="lineNum">     145 </span>            :     __asm__ __volatile__(&quot;mfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">     146 </span>            :   } else {
<span class="lineNum">     147 </span>            :     NoBarrier_AtomicExchange(ptr, value);
<span class="lineNum">     148 </span>            :                           // acts as a barrier on PIII
<span class="lineNum">     149 </span>            :   }
<span class="lineNum">     150 </span>            : }
<span class="lineNum">     151 </span>            : #endif
<span class="lineNum">     152 </span>            : 
<span class="lineNum">     153 </span>            : inline void Release_Store(volatile Atomic32* ptr, Atomic32 value) {
<span class="lineNum">     154 </span>            :   ATOMICOPS_COMPILER_BARRIER();
<span class="lineNum">     155 </span>            :   *ptr = value;  // An x86 store acts as a release barrier.
<span class="lineNum">     156 </span>            :   // See comments in Atomic64 version of Release_Store(), below.
<span class="lineNum">     157 </span>            : }
<span class="lineNum">     158 </span>            : 
<span class="lineNum">     159 </span>            : inline Atomic32 NoBarrier_Load(volatile const Atomic32* ptr) {
<span class="lineNum">     160 </span>            :   return *ptr;
<span class="lineNum">     161 </span>            : }
<span class="lineNum">     162 </span>            : 
<span class="lineNum">     163 </span>            : inline Atomic32 Acquire_Load(volatile const Atomic32* ptr) {
<span class="lineNum">     164 </span>            :   Atomic32 value = *ptr;  // An x86 load acts as a acquire barrier.
<span class="lineNum">     165 </span>            :   // See comments in Atomic64 version of Release_Store(), below.
<span class="lineNum">     166 </span>            :   ATOMICOPS_COMPILER_BARRIER();
<span class="lineNum">     167 </span>            :   return value;
<span class="lineNum">     168 </span>            : }
<span class="lineNum">     169 </span>            : 
<span class="lineNum">     170 </span>            : inline Atomic32 Release_Load(volatile const Atomic32* ptr) {
<span class="lineNum">     171 </span>            :   MemoryBarrier();
<span class="lineNum">     172 </span>            :   return *ptr;
<span class="lineNum">     173 </span>            : }
<span class="lineNum">     174 </span>            : 
<span class="lineNum">     175 </span>            : #if defined(__x86_64__)
<span class="lineNum">     176 </span>            : 
<a name="177"><span class="lineNum">     177 </span>            : // 64-bit low-level operations on 64-bit platform.</a>
<span class="lineNum">     178 </span>            : 
<span class="lineNum">     179 </span><span class="lineCov">         12 : inline Atomic64 NoBarrier_CompareAndSwap(volatile Atomic64* ptr,</span>
<span class="lineNum">     180 </span>            :                                          Atomic64 old_value,
<span class="lineNum">     181 </span>            :                                          Atomic64 new_value) {
<span class="lineNum">     182 </span>            :   Atomic64 prev;
<span class="lineNum">     183 </span>            :   __asm__ __volatile__(&quot;lock; cmpxchgq %1,%2&quot;
<span class="lineNum">     184 </span>            :                        : &quot;=a&quot; (prev)
<span class="lineNum">     185 </span>            :                        : &quot;q&quot; (new_value), &quot;m&quot; (*ptr), &quot;0&quot; (old_value)
<span class="lineNum">     186 </span><span class="lineCov">         12 :                        : &quot;memory&quot;);</span>
<span class="lineNum">     187 </span><span class="lineCov">         12 :   return prev;</span>
<span class="lineNum">     188 </span>            : }
<span class="lineNum">     189 </span>            : 
<span class="lineNum">     190 </span>            : inline Atomic64 NoBarrier_AtomicExchange(volatile Atomic64* ptr,
<span class="lineNum">     191 </span>            :                                          Atomic64 new_value) {
<span class="lineNum">     192 </span>            :   __asm__ __volatile__(&quot;xchgq %1,%0&quot;  // The lock prefix is implicit for xchg.
<span class="lineNum">     193 </span>            :                        : &quot;=r&quot; (new_value)
<span class="lineNum">     194 </span>            :                        : &quot;m&quot; (*ptr), &quot;0&quot; (new_value)
<span class="lineNum">     195 </span>            :                        : &quot;memory&quot;);
<span class="lineNum">     196 </span>            :   return new_value;  // Now it's the previous value.
<span class="lineNum">     197 </span>            : }
<span class="lineNum">     198 </span>            : 
<span class="lineNum">     199 </span>            : inline Atomic64 NoBarrier_AtomicIncrement(volatile Atomic64* ptr,
<span class="lineNum">     200 </span>            :                                           Atomic64 increment) {
<span class="lineNum">     201 </span>            :   Atomic64 temp = increment;
<span class="lineNum">     202 </span>            :   __asm__ __volatile__(&quot;lock; xaddq %0,%1&quot;
<span class="lineNum">     203 </span>            :                        : &quot;+r&quot; (temp), &quot;+m&quot; (*ptr)
<span class="lineNum">     204 </span>            :                        : : &quot;memory&quot;);
<span class="lineNum">     205 </span>            :   // temp now contains the previous value of *ptr
<span class="lineNum">     206 </span>            :   return temp + increment;
<span class="lineNum">     207 </span>            : }
<span class="lineNum">     208 </span>            : 
<span class="lineNum">     209 </span>            : inline Atomic64 Barrier_AtomicIncrement(volatile Atomic64* ptr,
<span class="lineNum">     210 </span>            :                                         Atomic64 increment) {
<span class="lineNum">     211 </span>            :   Atomic64 temp = increment;
<span class="lineNum">     212 </span>            :   __asm__ __volatile__(&quot;lock; xaddq %0,%1&quot;
<span class="lineNum">     213 </span>            :                        : &quot;+r&quot; (temp), &quot;+m&quot; (*ptr)
<span class="lineNum">     214 </span>            :                        : : &quot;memory&quot;);
<span class="lineNum">     215 </span>            :   // temp now contains the previous value of *ptr
<span class="lineNum">     216 </span>            :   if (AtomicOps_Internalx86CPUFeatures.has_amd_lock_mb_bug) {
<span class="lineNum">     217 </span>            :     __asm__ __volatile__(&quot;lfence&quot; : : : &quot;memory&quot;);
<span class="lineNum">     218 </span>            :   }
<span class="lineNum">     219 </span>            :   return temp + increment;
<span class="lineNum">     220 </span>            : }
<span class="lineNum">     221 </span>            : 
<span class="lineNum">     222 </span>            : inline void NoBarrier_Store(volatile Atomic64* ptr, Atomic64 value) {
<span class="lineNum">     223 </span>            :   *ptr = value;
<span class="lineNum">     224 </span>            : }
<span class="lineNum">     225 </span>            : 
<span class="lineNum">     226 </span>            : inline void Acquire_Store(volatile Atomic64* ptr, Atomic64 value) {
<span class="lineNum">     227 </span>            :   *ptr = value;
<span class="lineNum">     228 </span>            :   MemoryBarrier();
<a name="229"><span class="lineNum">     229 </span>            : }</a>
<span class="lineNum">     230 </span>            : 
<span class="lineNum">     231 </span><span class="lineCov">         12 : inline void Release_Store(volatile Atomic64* ptr, Atomic64 value) {</span>
<span class="lineNum">     232 </span><span class="lineCov">         12 :   ATOMICOPS_COMPILER_BARRIER();</span>
<span class="lineNum">     233 </span>            : 
<span class="lineNum">     234 </span><span class="lineCov">         12 :   *ptr = value;  // An x86 store acts as a release barrier</span>
<span class="lineNum">     235 </span>            :                  // for current AMD/Intel chips as of Jan 2008.
<span class="lineNum">     236 </span>            :                  // See also Acquire_Load(), below.
<span class="lineNum">     237 </span>            : 
<span class="lineNum">     238 </span>            :   // When new chips come out, check:
<span class="lineNum">     239 </span>            :   //  IA-32 Intel Architecture Software Developer's Manual, Volume 3:
<span class="lineNum">     240 </span>            :   //  System Programming Guide, Chatper 7: Multiple-processor management,
<span class="lineNum">     241 </span>            :   //  Section 7.2, Memory Ordering.
<span class="lineNum">     242 </span>            :   // Last seen at:
<span class="lineNum">     243 </span>            :   //   http://developer.intel.com/design/pentium4/manuals/index_new.htm
<span class="lineNum">     244 </span>            :   //
<span class="lineNum">     245 </span>            :   // x86 stores/loads fail to act as barriers for a few instructions (clflush
<span class="lineNum">     246 </span>            :   // maskmovdqu maskmovq movntdq movnti movntpd movntps movntq) but these are
<span class="lineNum">     247 </span>            :   // not generated by the compiler, and are rare.  Users of these instructions
<span class="lineNum">     248 </span>            :   // need to know about cache behaviour in any case since all of these involve
<span class="lineNum">     249 </span>            :   // either flushing cache lines or non-temporal cache hints.
<span class="lineNum">     250 </span><span class="lineCov">         12 : }</span>
<span class="lineNum">     251 </span>            : 
<span class="lineNum">     252 </span>            : inline Atomic64 NoBarrier_Load(volatile const Atomic64* ptr) {
<span class="lineNum">     253 </span>            :   return *ptr;
<a name="254"><span class="lineNum">     254 </span>            : }</a>
<span class="lineNum">     255 </span>            : 
<span class="lineNum">     256 </span><span class="lineCov">        696 : inline Atomic64 Acquire_Load(volatile const Atomic64* ptr) {</span>
<span class="lineNum">     257 </span><span class="lineCov">        696 :   Atomic64 value = *ptr;  // An x86 load acts as a acquire barrier,</span>
<span class="lineNum">     258 </span>            :                           // for current AMD/Intel chips as of Jan 2008.
<span class="lineNum">     259 </span>            :                           // See also Release_Store(), above.
<span class="lineNum">     260 </span><span class="lineCov">        696 :   ATOMICOPS_COMPILER_BARRIER();</span>
<span class="lineNum">     261 </span><span class="lineCov">        696 :   return value;</span>
<span class="lineNum">     262 </span>            : }
<span class="lineNum">     263 </span>            : 
<span class="lineNum">     264 </span>            : inline Atomic64 Release_Load(volatile const Atomic64* ptr) {
<span class="lineNum">     265 </span>            :   MemoryBarrier();
<span class="lineNum">     266 </span>            :   return *ptr;
<a name="267"><span class="lineNum">     267 </span>            : }</a>
<span class="lineNum">     268 </span>            : 
<span class="lineNum">     269 </span><span class="lineCov">         12 : inline Atomic64 Acquire_CompareAndSwap(volatile Atomic64* ptr,</span>
<span class="lineNum">     270 </span>            :                                        Atomic64 old_value,
<span class="lineNum">     271 </span>            :                                        Atomic64 new_value) {
<span class="lineNum">     272 </span><span class="lineCov">         12 :   Atomic64 x = NoBarrier_CompareAndSwap(ptr, old_value, new_value);</span>
<span class="lineNum">     273 </span><span class="lineCov">         12 :   if (AtomicOps_Internalx86CPUFeatures.has_amd_lock_mb_bug) {</span>
<span class="lineNum">     274 </span><span class="lineNoCov">          0 :     __asm__ __volatile__(&quot;lfence&quot; : : : &quot;memory&quot;);</span>
<span class="lineNum">     275 </span>            :   }
<span class="lineNum">     276 </span><span class="lineCov">         12 :   return x;</span>
<span class="lineNum">     277 </span>            : }
<span class="lineNum">     278 </span>            : 
<span class="lineNum">     279 </span>            : inline Atomic64 Release_CompareAndSwap(volatile Atomic64* ptr,
<span class="lineNum">     280 </span>            :                                        Atomic64 old_value,
<span class="lineNum">     281 </span>            :                                        Atomic64 new_value) {
<span class="lineNum">     282 </span>            :   return NoBarrier_CompareAndSwap(ptr, old_value, new_value);
<span class="lineNum">     283 </span>            : }
<span class="lineNum">     284 </span>            : 
<span class="lineNum">     285 </span>            : #endif  // defined(__x86_64__)
<span class="lineNum">     286 </span>            : 
<span class="lineNum">     287 </span>            : }  // namespace internal
<span class="lineNum">     288 </span>            : }  // namespace protobuf
<span class="lineNum">     289 </span>            : }  // namespace google
<span class="lineNum">     290 </span>            : 
<span class="lineNum">     291 </span>            : #undef ATOMICOPS_COMPILER_BARRIER
<span class="lineNum">     292 </span>            : 
<span class="lineNum">     293 </span>            : #endif  // GOOGLE_PROTOBUF_ATOMICOPS_INTERNALS_X86_GCC_H_
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../../../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.13</a></td></tr>
  </table>
  <br>

</body>
</html>
